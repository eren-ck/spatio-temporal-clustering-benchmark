{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bigger-richmond",
   "metadata": {},
   "source": [
    "## Spatio-Temporal Clustering Benchmark for Collective Animal Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-tsunami",
   "metadata": {},
   "source": [
    "In this notebook we benchmark various ST clustering algorithms. \n",
    "We further test their ability to operate on large amounts of data using their `fit_frame_split` method. Also, we implement grid search for spatial-temporal data.\n",
    "\n",
    "Clustering algorithms covered\n",
    "- ST DBSCAN\n",
    "- ST Agglomerative\n",
    "- ST KMeans\n",
    "- ST Optics\n",
    "- ST Spectral Clustering\n",
    "- ST Affinity Propagation\n",
    "- ST BIRCH\n",
    "- ST HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-humidity",
   "metadata": {},
   "source": [
    "### 00 Some helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "listed-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control execution time of functions\n",
    "import threading\n",
    "\n",
    "TIMER = 120\n",
    "PERMUT = 12\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "class InterruptableThread(threading.Thread):\n",
    "    def __init__(self, func, *args, **kwargs):\n",
    "        threading.Thread.__init__(self)\n",
    "        self._func = func\n",
    "        self._args = args\n",
    "        self._kwargs = kwargs\n",
    "        self._result = None\n",
    "\n",
    "    def run(self):\n",
    "        self._result = self._func(*self._args, **self._kwargs)\n",
    "\n",
    "    @property\n",
    "    def result(self):\n",
    "        return self._result\n",
    "\n",
    "\n",
    "class timeout(object):\n",
    "    def __init__(self, sec):\n",
    "        self._sec = sec\n",
    "\n",
    "    def __call__(self, f):\n",
    "        def wrapped_f(*args, **kwargs):\n",
    "            it = InterruptableThread(f, *args, **kwargs)\n",
    "            it.start()\n",
    "            it.join(self._sec)\n",
    "            if not it.is_alive():\n",
    "                return it.result\n",
    "            raise TimeoutError('execution expired')\n",
    "        return wrapped_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premium-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(parameters):\n",
    "    \"\"\"Helper function for st_grid_search. Returns a dictionary of all possible parameter combinations.\"\"\"\n",
    "    if not parameters:\n",
    "        yield dict()\n",
    "    else:\n",
    "        key_to_iterate = list(parameters.keys())[0]\n",
    "        next_round_parameters = {p : parameters[p]\n",
    "                    for p in parameters if p != key_to_iterate}\n",
    "        for val in parameters[key_to_iterate]:\n",
    "            for pars in make_generator(next_round_parameters):\n",
    "                temp_res = pars\n",
    "                temp_res[key_to_iterate] = val\n",
    "                yield temp_res\n",
    "                \n",
    "def st_silhouette_score(X, labels, eps1=0.05, eps2=10, metric='euclidean'):\n",
    "    \"\"\"Helper function for st_grid_search. Hyperparameter combinations are evaluated with the Silhouette score.\"\"\"\n",
    "    n, m = X.shape\n",
    "    time_dist = pdist(X[:, 0].reshape(n, 1), metric=metric)\n",
    "    euc_dist = pdist(X[:, 1:], metric=metric)\n",
    "\n",
    "    # filter the euc_dist matrix using the time_dist\n",
    "    dist = np.where(time_dist <= eps2, euc_dist, 2 * eps1)\n",
    "\n",
    "    return silhouette_score(squareform(dist), labels, metric='precomputed')\n",
    "\n",
    "@timeout(TIMER*PERMUT)\n",
    "def st_grid_search(estimator, split, X, param_dict, metric, y=None, frame_size=None, frame_overlap=None):\n",
    "    \"\"\"\n",
    "    Grid Search of hyperparameters for spatial-temporal clustering algorithms\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: class\n",
    "        ST clustering algorithm\n",
    "    split: boolean\n",
    "        Flag to indicate whether whole X should be loaded in RAM or processed in smaller chunks.\n",
    "    X: numpy array\n",
    "        Data on which grid search is performed\n",
    "    param_dict: dict\n",
    "        Dictionary with parameters to be optimized as keys and value range of grid search as value.\n",
    "    metric: str\n",
    "        The metric to evaluate the clustering quality\n",
    "    y: numpy array\n",
    "        Optional. Some metrics compare predictions with ground truth. Then, labels need to be provided.\n",
    "    frame_size: int\n",
    "        Optional. If split is True, indicate how large the chunks should be.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    param_opt\n",
    "        Optimal hyperparameter combination\n",
    "    \"\"\"\n",
    "    param_opt = None\n",
    "    s_max = 0\n",
    "    for param in make_generator(param_dict):\n",
    "        clust = estimator(**param)\n",
    "        if not split:\n",
    "            clust.st_fit(X)\n",
    "        else:\n",
    "            clust.st_fit_frame_split(X, frame_size, frame_overlap)\n",
    "            \n",
    "        if param_opt is None: \n",
    "            param_opt = param\n",
    "        \n",
    "        # different performance evaluation metrics\n",
    "        if metric=='silhouette':\n",
    "            try:\n",
    "                score = st_silhouette_score(X=X, labels=clust.labels, eps1=param['eps1'] , eps2=param['eps2'], metric='euclidean')\n",
    "            except (TypeError, ValueError) as e:\n",
    "                continue\n",
    "            #print('Silhouette score for parameters {}: {}'.format(param,score))\n",
    "        elif metric=='ami':\n",
    "            score = adjusted_mutual_info_score(y,clust.labels)\n",
    "\n",
    "        # store parameter combination if it outperforms given the metric\n",
    "        if score > s_max:\n",
    "            s_max = score\n",
    "            param_opt = param\n",
    "    return param_opt\n",
    "\n",
    "@timeout(TIMER*PERMUT)\n",
    "def traj_grid_search(estimator, X, param_dict, metric):\n",
    "    \"\"\"\n",
    "    Grid Search of hyperparameters for spatial-temporal clustering algorithms\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: class\n",
    "        ST clustering algorithm\n",
    "    split: boolean\n",
    "        Flag to indicate whether whole X should be loaded in RAM or processed in smaller chunks.\n",
    "    X: numpy array\n",
    "        Data on which grid search is performed\n",
    "    param_dict: dict\n",
    "        Dictionary with parameters to be optimized as keys and value range of grid search as value.\n",
    "    metric: str\n",
    "        The metric to evaluate the clustering quality\n",
    "    y: numpy array\n",
    "        Optional. Some metrics compare predictions with ground truth. Then, labels need to be provided.\n",
    "    frame_size: int\n",
    "        Optional. If split is True, indicate how large the chunks should be.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    param_opt\n",
    "        Optimal hyperparameter combination\n",
    "    \"\"\"\n",
    "    param_opt = {'detect_radius':40, 'similarity_threshold':0.5}\n",
    "    s_max = 0\n",
    "    for param in make_generator(param_dict):\n",
    "        clust = estimator(**param)\n",
    "        clust.st_fit(X)\n",
    "        \n",
    "        if param_opt is None: \n",
    "            param_opt = param\n",
    "        \n",
    "        # different performance evaluation metrics\n",
    "        if metric=='silhouette':\n",
    "            try:\n",
    "                score = st_silhouette_score(X=X, labels=clust.labels, eps1=param['eps1'] , eps2=param['eps2'], metric='euclidean')\n",
    "            except (TypeError, ValueError) as e:\n",
    "                continue\n",
    "            #print('Silhouette score for parameters {}: {}'.format(param,score))\n",
    "        elif metric=='ami':\n",
    "            score = adjusted_mutual_info_score(clust.true_labels,clust.labels)\n",
    "            #print('AMI score for parameters {}: {}'.format(param,score))\n",
    "            \n",
    "        # store parameter combination if it outperforms given the metric\n",
    "        if score > s_max:\n",
    "            s_max = score\n",
    "            param_opt = param\n",
    "    return param_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stable-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(object):       \n",
    "    # use this function with st clusterers\n",
    "    @timeout(TIMER) # set seconds for timeout\n",
    "    def frame_split_cluster(self, algorithm, data, frame_size, frame_overlap):\n",
    "        start_time = time.time()\n",
    "        algorithm.st_fit_frame_split(data, frame_size, frame_overlap)\n",
    "        runtime = time.time() - start_time\n",
    "        ami = adjusted_mutual_info_score(labels, algorithm.labels)\n",
    "        return ami, runtime\n",
    "        \n",
    "    # use this with trajectory clustering\n",
    "    @timeout(TIMER)\n",
    "    def traj_cluster(self,algorithm, data):\n",
    "        start_time = time.time()\n",
    "        algorithm.st_fit(data)\n",
    "        runtime = time.time() - start_time\n",
    "        ami = adjusted_mutual_info_score(algorithm.true_labels, algorithm.labels)\n",
    "        return ami, runtime\n",
    "        \n",
    "    # use this with dbscan2\n",
    "    @timeout(TIMER)\n",
    "    def cluster(self, algorithm, data):\n",
    "        start_time = time.time()\n",
    "        algorithm.st_fit(data)\n",
    "        runtime = time.time() - start_time\n",
    "        ami = adjusted_mutual_info_score(labels, algorithm.labels)\n",
    "        return ami, runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-spine",
   "metadata": {},
   "source": [
    "### 01 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "honest-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import silhouette_score, adjusted_mutual_info_score\n",
    "from st_clustering import ST_DBSCAN, ST_Agglomerative, ST_KMeans, ST_OPTICS, ST_SpectralClustering, ST_AffinityPropagation, ST_BIRCH, ST_HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pleased-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging of results\n",
    "logging.basicConfig(level=logging.INFO, filename='cluster_results.log', filemode='a', format='%(asctime)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-lying",
   "metadata": {},
   "source": [
    "### 02 Find Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-insurance",
   "metadata": {},
   "source": [
    "Procedure: \n",
    "1. Extensive search on small dataset. \n",
    "2. Define a smaller search space around optimal values from step 1.\n",
    "3. Search on larger dataset for values from step 2.\n",
    "4. Evaluate if optimal hyperparameters from step 3 are extreme values (min or max of search space). If not, search space is properly defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-decline",
   "metadata": {},
   "source": [
    "### 03 Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-quantity",
   "metadata": {},
   "source": [
    "Loop to iterate over datasets\n",
    "\n",
    "* For each dataset\n",
    "    * Extract name\n",
    "    * For each clusterer\n",
    "         * find hyperparameter\n",
    "         * cluster and write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "golden-ottawa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = 'test_files'\n",
    "dataset_files = os.listdir(PATH)\n",
    "\n",
    "FRAME_SIZE = 100\n",
    "FRAME_OVERLAP = 10\n",
    "\n",
    "t = Test()\n",
    "\n",
    "# flags set to false if timeouterror occured. we need to avoid running into the same error \n",
    "not_timed_out_dbscan = True\n",
    "not_timed_out_agglo = True\n",
    "not_timed_out_kmeans = True\n",
    "# not_timed_out_optics = True\n",
    "# not_timed_out_spectral = True\n",
    "# not_timed_out_affinity = True\n",
    "not_timed_out_birch = True\n",
    "not_timed_out_hdbscan = True\n",
    "\n",
    "for ds in dataset_files:\n",
    "    # read data\n",
    "    filename = ds\n",
    "    df = pd.read_csv(PATH+\"/\"+filename)\n",
    "    df['x'] = (df['x'] - df['x'].min()) / (df['x'].max() - df['x'].min())\n",
    "    df['y'] = (df['y'] - df['y'].min()) / (df['y'].max() - df['y'].min())\n",
    "    # transform to numpy array\n",
    "    data = df.loc[:, ['frame','x','y']].values\n",
    "    labels = df['cid'].to_numpy()\n",
    "    \n",
    "    # get number of clusters\n",
    "    if -1 in np.unique(labels):\n",
    "        n_cluster = len(np.unique(labels)) - 1\n",
    "    else:\n",
    "        n_cluster = len(np.unique(labels))\n",
    "    \n",
    "    # grid search\n",
    "    param_dict_dbscan = {'eps1': [0.02, 0.03, 0.04, 0.05],'eps2':[5, 25, 50, 100], 'min_samples': [2]}\n",
    "    param_dict_agglo = {'eps2':[5, 25, 50, 100], 'n_clusters': [n_cluster]}\n",
    "    param_dict_kmeans = {'eps2':[5, 25, 50, 100], 'n_clusters': [n_cluster]}\n",
    "    # param_dict_optics = {'eps2':[5, 15, 25], 'min_cluster_size': [2], 'min_samples' : [2]}\n",
    "    # param_dict_spectral = {'eps2':[5, 15, 25], 'n_clusters': [n_cluster]}\n",
    "    # param_dict_affinity = {'eps2':[5, 15, 25]}\n",
    "    param_dict_birch = {'eps2':[5, 25, 50, 100], 'threshold': [0.3, 0.5], 'n_clusters': [n_cluster]}\n",
    "    param_dict_hdbscan = {'eps2':[5, 25, 50, 100], 'min_cluster_size': [n_cluster], 'min_samples': [2]}\n",
    "    ## ---\n",
    "    \n",
    "    \n",
    "    # cluster and write results to log file\n",
    "    \n",
    "    ## ST_DBSCAN \n",
    "    if not_timed_out_dbscan:\n",
    "        try:\n",
    "            opt_param_dbscan = st_grid_search(estimator=ST_DBSCAN, split=True, X=data, param_dict=param_dict_dbscan, metric='ami', y=labels, frame_size=FRAME_SIZE, frame_overlap=FRAME_OVERLAP)\n",
    "            dbscan = ST_DBSCAN(**opt_param_dbscan)\n",
    "            ami, runtime = t.frame_split_cluster(dbscan, data, FRAME_SIZE, FRAME_OVERLAP)\n",
    "            logging.info('dataset: {}, method: {}, ami: {}, execution time: {}'.format(filename, dbscan, ami, runtime))\n",
    "        except TimeoutError:\n",
    "            logging.info('TimeoutError! dataset: {}, method: {}'.format(filename, 'DBSCAN'))\n",
    "            not_timed_out_dbscan = False\n",
    "            pass\n",
    "        except:\n",
    "            logging.info('ComputationalError! dataset: {}, method: {}'.format(filename, 'DBSCAN'))\n",
    "            pass\n",
    "    \n",
    "    ## ST_Agglomerative \n",
    "    if not_timed_out_agglo:\n",
    "        try:\n",
    "            opt_param_agglo = st_grid_search(estimator=ST_Agglomerative, split=True, X=data, param_dict=param_dict_agglo, metric='ami', y=labels, frame_size=FRAME_SIZE, frame_overlap=FRAME_OVERLAP)\n",
    "            agglo = ST_Agglomerative(**opt_param_agglo)\n",
    "            ami, runtime = t.frame_split_cluster(agglo, data, FRAME_SIZE, FRAME_OVERLAP)\n",
    "            logging.info('dataset: {}, method: {}, ami: {}, execution time: {}'.format(filename, agglo, ami, runtime))\n",
    "        except TimeoutError:\n",
    "            logging.info('TimeoutError! dataset: {}, method: {}'.format(filename, 'Agglomerative'))\n",
    "            not_timed_out_agglo = False\n",
    "            pass\n",
    "        except:\n",
    "            logging.info('ComputationalError! dataset: {}, method: {}'.format(filename, 'Agglomerative'))\n",
    "            pass\n",
    "    \n",
    "    ## K-MEANS\n",
    "    if not_timed_out_kmeans:\n",
    "        try:\n",
    "            opt_param_kmeans = st_grid_search(estimator=ST_KMeans, split=True, X=data, param_dict=param_dict_kmeans, metric='ami', y=labels, frame_size=FRAME_SIZE, frame_overlap=FRAME_OVERLAP)\n",
    "            kmeans = ST_KMeans(**opt_param_kmeans)\n",
    "            ami, runtime = t.frame_split_cluster(kmeans, data, FRAME_SIZE, FRAME_OVERLAP)\n",
    "            logging.info('dataset: {}, method: {}, ami: {}, execution time: {}'.format(filename, kmeans, ami, runtime))\n",
    "        except TimeoutError:\n",
    "            logging.info('TimeoutError! dataset: {}, method: {}'.format(filename, 'KMeans'))\n",
    "            not_timed_out_kmeans = False\n",
    "            pass\n",
    "        except:\n",
    "            logging.info('ComputationalError! dataset: {}, method: {}'.format(filename, 'KMeans'))\n",
    "            pass\n",
    "    \n",
    "    ## BIRCH\n",
    "    if not_timed_out_birch:\n",
    "        try:\n",
    "            opt_param_birch = st_grid_search(estimator=ST_BIRCH, split=True, X=data, param_dict=param_dict_birch, metric='ami', y=labels, frame_size=FRAME_SIZE, frame_overlap=FRAME_OVERLAP)\n",
    "            birch = ST_BIRCH(**opt_param_birch)\n",
    "            ami, runtime = t.frame_split_cluster(birch, data, FRAME_SIZE, FRAME_OVERLAP)\n",
    "            logging.info('dataset: {}, method: {}, ami: {}, execution time: {}'.format(filename, birch, ami, runtime))\n",
    "        except TimeoutError:\n",
    "            logging.info('TimeoutError! dataset: {}, method: {}'.format(filename, 'BIRCH'))\n",
    "            not_timed_out_birch = False\n",
    "            pass\n",
    "        except:\n",
    "            logging.info('ComputationalError! dataset: {}, method: {}'.format(filename, 'BIRCH'))\n",
    "            pass\n",
    "        \n",
    "    ## HDBSCAN  \n",
    "    if not_timed_out_hdbscan:\n",
    "        try:\n",
    "            opt_param_hdbscan = st_grid_search(estimator=ST_HDBSCAN, split=True, X=data, param_dict=param_dict_birch, metric='ami', y=labels, frame_size=FRAME_SIZE, frame_overlap=FRAME_OVERLAP)\n",
    "            hdbscan = ST_HDBSCAN(**opt_param_hdbscan)\n",
    "            ami, runtime = t.frame_split_cluster(hdbscan, data, FRAME_SIZE, FRAME_OVERLAP)\n",
    "            logging.info('dataset: {}, method: {}, ami: {}, execution time: {}'.format(filename, hdbscan, ami, runtime))\n",
    "        except TimeoutError:\n",
    "            logging.info('TimeoutError! dataset: {}, method: {}'.format(filename, 'HDBSCAN'))\n",
    "            not_timed_out_hdbscan = False\n",
    "            pass \n",
    "        except:\n",
    "            logging.info('ComputationalError! dataset: {}, method: {}'.format(filename, 'HDBSCAN'))\n",
    "            pass\n",
    "        \n",
    "    ## OPTICS \n",
    "    # if not_timed_out_optics:\n",
    "    #     try:\n",
    "    #         opt_param_optics = st_grid_search(estimator=ST_OPTICS, split=True, X=data, param_dict=param_dict_optics, metric='ami', y=labels, frame_size=FRAME_SIZE, frame_overlap=FRAME_OVERLAP)\n",
    "    #         optics = ST_OPTICS(**opt_param_optics)\n",
    "    #         ami, runtime = t.frame_split_cluster(optics, data, FRAME_SIZE, FRAME_OVERLAP)\n",
    "    #         logging.info('dataset: {}, method: {}, ami: {}, execution time: {}'.format(filename, optics, ami, runtime))\n",
    "    #     except TimeoutError:\n",
    "    #         logging.info('TimeoutError! dataset: {}, method: {}'.format(filename, 'OPTICS'))\n",
    "    #         not_timed_out_optics = False\n",
    "    #         pass\n",
    "    #     except:\n",
    "    #         logging.info('ComputationalError! dataset: {}, method: {}'.format(filename, 'OPTICS'))\n",
    "    #         pass\n",
    "    \n",
    "    ## SPECTRAL \n",
    "    # if not_timed_out_spectral:\n",
    "    #     try:\n",
    "    #         opt_param_spectral = st_grid_search(estimator=ST_SpectralClustering, split=True, X=data, param_dict=param_dict_spectral, metric='ami', y=labels, frame_size=FRAME_SIZE, frame_overlap=FRAME_OVERLAP)\n",
    "    #         spectral = ST_SpectralClustering(**opt_param_spectral)\n",
    "    #         ami, runtime = t.frame_split_cluster(spectral, data, FRAME_SIZE,FRAME_OVERLAP)\n",
    "    #         logging.info('dataset: {}, method: {}, ami: {}, execution time: {}'.format(filename, spectral, ami, runtime))\n",
    "    #     except TimeoutError:\n",
    "    #         logging.info('TimeoutError! dataset: {}, method: {}'.format(filename, 'Spectral'))\n",
    "    #         not_timed_out_spectral = False\n",
    "    #         pass\n",
    "    #     except:\n",
    "    #         logging.info('ComputationalError! dataset: {}, method: {}'.format(filename, 'Spectral'))\n",
    "    #         pass\n",
    "        \n",
    "    ## AFFINITY \n",
    "    # if not_timed_out_affinity:\n",
    "    #     try:\n",
    "    #         opt_param_affinity = st_grid_search(estimator=ST_AffinityPropagation, split=True, X=data, param_dict=param_dict_affinity, metric='ami', y=labels, frame_size=FRAME_SIZE, frame_overlap=FRAME_OVERLAP)\n",
    "    #         affinity = ST_AffinityPropagation(**opt_param_affinity)\n",
    "    #         ami, runtime = t.frame_split_cluster(affinity, data, FRAME_SIZE, FRAME_OVERLAP)\n",
    "    #         logging.info('dataset: {}, method: {}, ami: {}, execution time: {}'.format(filename, affinity,  ami, runtime))\n",
    "    #     except TimeoutError:\n",
    "    #         logging.info('TimeoutError! dataset: {}, method: {}'.format(filename, 'Affinity'))\n",
    "    #         not_timed_out_affinity = False\n",
    "    #         pass\n",
    "    #     except:\n",
    "    #         logging.info('ComputationalError! dataset: {}, method: {}'.format(filename, 'Affinity'))\n",
    "    #         pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
